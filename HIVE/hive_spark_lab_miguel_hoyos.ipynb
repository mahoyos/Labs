{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66465db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 did not start up in 60 seconds..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc012ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 did not start up in 60 seconds..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "162f290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>4</td><td>application_1731543739540_0007</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-33-213.ec2.internal:20888/proxy/application_1731543739540_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-39-150.ec2.internal:8042/node/containerlogs/container_1731543739540_0007_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4f26604\n"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2330a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: org.apache.spark.SparkContext = org.apache.spark.SparkContext@2c2aa2e5\n"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f361e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+------+-----+-----+-----+\n",
      "|  id|             country|  hdi|lifeex|mysch|eysch|  gni|\n",
      "+----+--------------------+-----+------+-----+-----+-----+\n",
      "|NULL|             country| NULL|  NULL| NULL| NULL| NULL|\n",
      "|   1|              Norway|0.943|    81|   12|   17|47557|\n",
      "|   2|           Australia|0.929|    81|   12|   18|34431|\n",
      "|   3|         Netherlands| 0.91|    80|   11|   16|36402|\n",
      "|   4|       United States| 0.91|    78|   12|   16|43017|\n",
      "|   5|         New Zealand|0.908|    80|   12|   18|23737|\n",
      "|   6|              Canada|0.908|    81|   12|   16|35166|\n",
      "|   7|             Ireland|0.908|    80|   11|   18|29322|\n",
      "|   8|       Liechtenstein|0.905|    79|   10|   14|83717|\n",
      "|   9|             Germany|0.905|    80|   12|   15|34854|\n",
      "|  10|              Sweden|0.904|    81|   11|   15|35837|\n",
      "|  11|         Switzerland|0.903|    82|   11|   15|39924|\n",
      "|  12|               Japan|0.901|    83|   11|   15|32295|\n",
      "|  13|Hong Kong China (...|0.898|    82|   10|   15|44805|\n",
      "|  14|             Iceland|0.898|    81|   10|   18|29354|\n",
      "|  15| Korea (Republic of)|0.897|    80|   11|   16|28230|\n",
      "|  16|             Denmark|0.895|    78|   11|   16|34347|\n",
      "|  17|              Israel|0.888|    81|   11|   15|25849|\n",
      "|  18|             Belgium|0.886|    80|   10|   16|33357|\n",
      "|  19|             Austria|0.885|    80|   10|   15|35719|\n",
      "+----+--------------------+-----+------+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hdi\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe383f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             country|  gni|\n",
      "+--------------------+-----+\n",
      "|              Norway|47557|\n",
      "|           Australia|34431|\n",
      "|         Netherlands|36402|\n",
      "|       United States|43017|\n",
      "|         New Zealand|23737|\n",
      "|              Canada|35166|\n",
      "|             Ireland|29322|\n",
      "|       Liechtenstein|83717|\n",
      "|             Germany|34854|\n",
      "|              Sweden|35837|\n",
      "|         Switzerland|39924|\n",
      "|               Japan|32295|\n",
      "|Hong Kong China (...|44805|\n",
      "|             Iceland|29354|\n",
      "| Korea (Republic of)|28230|\n",
      "|             Denmark|34347|\n",
      "|              Israel|25849|\n",
      "|             Belgium|33357|\n",
      "|             Austria|35719|\n",
      "|              France|30462|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select country, gni from hdi where gni > 2000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ed7ddef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "java.io.IOException: Path: //mahoyosv/gutenbergdataset is a directory, which is not supported by the record reader when `mapreduce.input.fileinputformat.input.dir.recursive` is false.\n",
      "  at org.apache.spark.errors.SparkCoreErrors$.pathNotSupportedError(SparkCoreErrors.scala:95)\n",
      "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:245)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:295)\n",
      "  at scala.Option.getOrElse(Option.scala:189)\n",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:291)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:58)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:295)\n",
      "  at scala.Option.getOrElse(Option.scala:189)\n",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:291)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:58)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:295)\n",
      "  at scala.Option.getOrElse(Option.scala:189)\n",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:291)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:58)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:295)\n",
      "  at scala.Option.getOrElse(Option.scala:189)\n",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:291)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:58)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:295)\n",
      "  at scala.Option.getOrElse(Option.scala:189)\n",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:291)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:58)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:295)\n",
      "  at scala.Option.getOrElse(Option.scala:189)\n",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:291)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2564)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1050)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:152)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:113)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:1049)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:496)\n",
      "  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.org$apache$spark$sql$execution$exchange$BroadcastExchangeExec$$doComputeRelation(BroadcastExchangeExec.scala:198)\n",
      "  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1.doCompute(BroadcastExchangeExec.scala:191)\n",
      "  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1.doCompute(BroadcastExchangeExec.scala:184)\n",
      "  at org.apache.spark.sql.execution.AsyncDriverOperation.$anonfun$compute$1(AsyncDriverOperation.scala:75)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:264)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:256)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionId$1(SQLExecution.scala:239)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:285)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:236)\n",
      "  at org.apache.spark.sql.execution.AsyncDriverOperation.compute(AsyncDriverOperation.scala:69)\n",
      "  at org.apache.spark.sql.execution.AsyncDriverOperation.$anonfun$computeFuture$1(AsyncDriverOperation.scala:55)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:308)\n",
      "  at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:303)\n",
      "  at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "  at java.base/java.lang.Thread.run(Thread.java:840)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:285)\n",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:284)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:573)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:535)\n",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4401)\n",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3360)\n",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4391)\n",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:711)\n",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4389)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:264)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:138)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:174)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:264)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$8(SQLExecution.scala:174)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:285)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:173)\n",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:70)\n",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4389)\n",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:3360)\n",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:3583)\n",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\n",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:318)\n",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:883)\n",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:842)\n",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:851)\n",
      "  ... 51 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT h.country, h.gni, e.expct FROM HDI h JOIN EXPO e ON h.country = e.country WHERE h.gni > 2000\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65e49bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set(\"mapreduce.input.fileinputformat.input.dir.recursive\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b2fbdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     word|count|\n",
      "+---------+-----+\n",
      "|?schines,|    1|\n",
      "|   zigzag|    1|\n",
      "|     zest|    1|\n",
      "|   zenith|    1|\n",
      "|zealously|    1|\n",
      "| zealous,|    1|\n",
      "|  zealous|    5|\n",
      "|    zeal,|    3|\n",
      "|     zeal|    8|\n",
      "| youthful|    2|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT word, count(1) AS count FROM (SELECT explode(split(line,' ')) AS word FROM docs) w GROUP BY word ORDER BY word DESC LIMIT 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "302d8ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the|44647|\n",
      "|  of|28020|\n",
      "|    |27298|\n",
      "|  to|23208|\n",
      "| and|20444|\n",
      "|  in|13174|\n",
      "|that|12265|\n",
      "|   I|10880|\n",
      "|   a|10431|\n",
      "|  is| 7776|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT word, count(1) AS count FROM (SELECT explode(split(line,' ')) AS word FROM docs) w GROUP BY word ORDER BY count DESC LIMIT 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622abbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
